Machine Learning End of Course Project

The goal of the project is to determine how well participants performed weight lifting excercise. A sensor was attached to participant body, that measured and recorded how well each participant performed each exercise. Performance was rated from A – E. There were 19,622 records in training set and 20 records in test set with160 variables. Data can be found here
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv;
https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv
More information about study can be found on this website:
http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset)

I read data with readr package and read_csv function. The first task was to use summary(), head() and str() functions to understand data. Summary() showed there were many variables with values = NA. Next I used dplyr package and ggplot2 packages to summarize data by user_name and classe. Classe is the grade assigned to performance for each activity. User_name was the identifier for each participant, there were 6 participants. Table below summarizes each user grade from A-E. 

  > sumtrain
            A   B   C   D   E
adelmo   1165 776 750 515 686
carlitos  834 690 493 486 609
charles   899 745 539 642 711
eurico    865 592 489 582 542
jeremy   1177 489 652 522 562
pedro     640 505 499 469 497

Also see MLProjectPlot1.png and MLProjectPlot2.png for graphs of distribution of user_name and classe variables

Next I used the nzv() function to exclude some variables, this resulted in training set with 56 variables but test set with 59 variables. I looked at details of the 3 variables in the test set that was excluded from the training set, some had negative values. I decided to also exclude them from the training set , with the intention of revisiting if prediction had low accuracy without them. Said variables excluded are Magnet_dumbell_z, magnet_forearm_y and magnet_forearm_z. I also excluded the X variable from both test and training set. These variables are index variables and will not add value to prediction. I finally ended up with 54 predictors (see Appendix III) plus classe variable – the response variable.

My plan was to use a decision tree, random forest and boosting or a combination to get 99% accuracy. I would train on cleansed data and use the 3 methods or a combination. Only employ cross validation, principal process analysis and scaling and centering of the predictors only if I did not get over 99% accuracy without them

I started with decision tree, it only gave 70%  accuracy in training set, using the Caret package train(). Confusion matrix, shows a lot of inaccuracy across all scores.  
 
 > confusionMatrix(rpart)
Bootstrapped (25 reps) Confusion Matrix 

(entries are percentual average cell counts across resamples)
 
          Reference
Prediction    A    B    C    D    E
         A 23.4  4.2  1.2  1.6  0.4
         B  1.2 10.4  0.7  0.3  1.2
         C  2.5  3.9 14.1  4.3  1.4
         D  1.0  0.8  1.1  9.0  2.1
         E  0.4  0.0  0.2  1.2 13.1
                            
 Accuracy (average) : 0.7002

Also, rpart$finalModel and decision tree plot (see WeightTrainTree2.png) only show predictor roll_belt variable for predicting. Train(method '”rpart”) ignored all other variables. But the varImp() on the model shows the following variable importance:

> varImp(rpart)
rpart variable importance

  only 20 most important variables shown (out of 76)

                                 Overall
roll_belt                         100.00
pitch_forearm                      73.05
accel_belt_z                       61.15
magnet_belt_y                      57.99
total_accel_belt                   50.23
yaw_forearm                         0.00

I thought decision tree plot and varImp() results were a contradiction. I also got very different results if I use rpart() vs train(method=”rpart”) for training.  At this point I decided to go on to random forest and boosting. 

Next I trained data using Caret package train() for random forests. To decrease run time, I used traincontrol() with train(). The result was 99.9% accuracy and .03% error rate on the training set, 

> rf
Random Forest 

19622 samples
   54 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 17660, 17661, 17659, 17660, 17658, 17660, ... 
Resampling results across tuning parameters:

  mtry  Accuracy   Kappa    
   2    0.9910819  0.9887181
  39    0.9995414  0.9994199
  76    0.9995414  0.9994199

I thought result was pretty good and no need to adjust predictors or use any other method if prediction on test set was up to 95% accurate.  Next I looked at the important variables(appendix I), just to have an idea what was driving prediction. Also, if I needed to revisit Decision Tree, it would give me an idea what variables to include/exclude. 
Since accuracy was over 99% with random forest on training set, I thought data was clean and standardized enough, no need for further processing. Next I used the predict() to make predictions on the test set, the result was;

 [1] B A B A A E D B A A B C B A E E A B B B
Levels: A B C D E

Actual user performance was not included in test set, I took quiz to see how good the predictions were, the result- 20 out of 20 predictions were correct. Though test set sample size was only 20, 100% accuracy was impressive. I decided random forest was a good model to predict this exercise and no further refinement of training set was needed.

For completion, and comparison between random forest and boosting I decided to predict with Caret package boosting and method =”gbm”. Boosting accuracy was 99.6% on training set. 

> gbm
Stochastic Gradient Boosting 

19622 samples
   54 predictor
    5 classes: 'A', 'B', 'C', 'D', 'E' 

No pre-processing
Resampling: Bootstrapped (25 reps) 
Summary of sample sizes: 19622, 19622, 19622, 19622, 19622, 19622, ... 
Resampling results across tuning parameters:

  interaction.depth  n.trees  Accuracy   Kappa    
  1                   50      0.8349587  0.7906257
  1                  100      0.8926579  0.8640909
  1                  150      0.9221697  0.9014436
  2                   50      0.9531231  0.9406699
  2                  100      0.9859655  0.9822484
  2                  150      0.9920218  0.9899092
  3                   50      0.9833775  0.9789730
  3                  100      0.9935119  0.9917939
  3                  150      0.9963097  0.9953327


First 20 Importance variables were the same for boosting and random forest. Finally, I got boosting prediction on the test set and compared to random forest result. Both boosting and random forest prediction were the same and 100% accurate on test set (appendix II)

In conclusion, prediction with Caret package Random Forest or Boosting(gbm) gave over 99% accuracy on training set and 100% accuracy on test set. See appendix III for predictors used to train models and ML_wk4_projectB.Rmd for R code.

                  Appendix
Appendix I – Random Forest Important variables
  only 20 most important variables shown (out of 76)

                               Overall
raw_timestamp_part_1           100.000
num_window                      50.542
roll_belt                       42.875
pitch_forearm                   26.347
pitch_belt                      17.075
magnet_dumbbell_y               16.592
roll_forearm                    15.175
yaw_belt                        14.151
cvtd_timestamp30/11/2011 17:12  11.284
cvtd_timestamp28/11/2011 14:15   9.890
cvtd_timestamp02/12/2011 14:58   9.688
cvtd_timestamp02/12/2011 13:33   7.513
accel_belt_z                     7.217
cvtd_timestamp05/12/2011 11:24   6.721
magnet_dumbbell_x                6.688
cvtd_timestamp02/12/2011 13:35   5.745
accel_forearm_x                  5.661
accel_dumbbell_y                 5.647
cvtd_timestamp02/12/2011 13:34   5.433
roll_dumbbell                    5.334

Appendix II Random Forest vs Boosting(gbm) Prediction
z<-cbind(rfpred,gbmPred)
z
        rfpred gbmPred
 [1,]      2       2
 [2,]      1       1
 [3,]      2       2
 [4,]      1       1
 [5,]      1       1
 [6,]      5       5
 [7,]      4       4
 [8,]      2       2
 [9,]      1       1
[10,]      1       1
[11,]      2       2
[12,]      3       3
[13,]      2       2
[14,]      1       1
[15,]      5       5
[16,]      5       5
[17,]      1       1
[18,]      2       2
[19,]      2       2
[20,]      2       2

Appendix III – Predictors used in Training Set
 colnames(training)
 [1] "user_name"            "raw_timestamp_part_1" "raw_timestamp_part_2"
 [4] "cvtd_timestamp"       "num_window"           "roll_belt"           
 [7] "pitch_belt"           "yaw_belt"             "total_accel_belt"    
[10] "gyros_belt_x"         "gyros_belt_y"         "gyros_belt_z"        
[13] "accel_belt_x"         "accel_belt_y"         "accel_belt_z"        
[16] "magnet_belt_x"        "magnet_belt_y"        "magnet_belt_z"       
[19] "roll_arm"             "pitch_arm"            "yaw_arm"             
[22] "total_accel_arm"      "gyros_arm_x"          "gyros_arm_y"         
[25] "gyros_arm_z"          "accel_arm_x"          "accel_arm_y"         
[28] "accel_arm_z"          "magnet_arm_x"         "magnet_arm_y"        
[31] "magnet_arm_z"         "roll_dumbbell"        "pitch_dumbbell"      
[34] "yaw_dumbbell"         "total_accel_dumbbell" "gyros_dumbbell_x"    
[37] "gyros_dumbbell_y"     "gyros_dumbbell_z"     "accel_dumbbell_x"    
[40] "accel_dumbbell_y"     "accel_dumbbell_z"     "magnet_dumbbell_x"   
[43] "magnet_dumbbell_y"    "roll_forearm"         "pitch_forearm"       
[46] "yaw_forearm"          "total_accel_forearm"  "gyros_forearm_x"     
[49] "gyros_forearm_y"      "gyros_forearm_z"      "accel_forearm_x"     
[52] "accel_forearm_y"      "accel_forearm_z"      "magnet_forearm_x"


file:///Users/angelina/Library/Application%20Support/LibreOffice/4/user/temp/lubeqd16.tmp/lubeqd4j.tmp/ML%20end%20of%20Course%20project.htm

